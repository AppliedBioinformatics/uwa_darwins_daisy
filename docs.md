# Darwin's Daisy Project, Taken over from Gagan 20/06/2025.

---
## Initial handover notes.

I met with Dave, A few of the postdocs and Gagan to discuss handover of the project on the 20/06/2025. To discuss the 
further work needed for the project. We came up with a list of tasks that needed doing. These are listed below:

* Generate a plot showing number of genes called as "present" against read depth for each sample. The curve should look 
like a cliff, with read depth tapering off as you move from right to left through the curve. This will help us to see 
which samples need fixing.

* Run masurca annotation with the original genome. Will need to look into how the original genome was annotated. For
help with maker, I can ask Tessa. She has lots of experience with it.

* Call presence/absence annotation. This can be done in parallel to maker.
* Gene ontology analysis.
* Map unmapped contigs back to the reference genome (the unmapped contigs have already been filtered). - Apparently 
Teng can help me with this.
* Look for a multiqc report to check quality of each sample and ensure that we are confident with all samples before 
moving forward.

## Assessing quality of samples.
I downloaded the multiqc report Gagan generated to look over and see if it could explain why some samples are showing
a large amount of gene loss. The multiqc report is [here](reports/initial_multiqc_report.html). The report showed that
despite passing quality thresholds, the number of total reads for some samples were very low, this means that the
depth of coverage of the genomes for these samples may not be good enough to cover all the present genes and hence 
would lead to the large loss of genes Gagan observed in her thesis. This was confirmed in the 
[mapping report](reports/mapping_multiqc_report.html) that showed that all samples had a high rate of alignment but 
likely do not have a good depth of coverage.

To a suitable threshold for suitable depth, The read depth plot below will be informative. It may even be informative
to show raw read count per sample against number of genes called as present.

## Generating present genes / read depth plot.
Gagan's sorted .bam folder only contains 26 bam files. I am going to use the raw_bams to complete the merge, as 
there should be 35 samples according to the [metadata](metadata/raw_sample_metadata.xlsx)

To Generate the plot I will complete the following workflow:
* Get read depth per .bam file using `samtools view -c -F 260 <sample>.bam`. This will get the number of mapped reads.
* Count genes per .ba, using `featureCounts -a .gff -o counts.txt <sample>.bam`
* Define "Present". I.e what threshold should determine if a gene is classed as present or not.
* Combine results into a table with columns `sample`, `Mapped read count`, and `Genes present`.
* Plot with Python.

Things to think about.
To assess how gene detection behaves at lower depths within the same sample, I could take a subsample of each of the 
.bams.

### Update 30/06/2025.
I finished merging the bam files and running SGSGeneLoss on the samples on the date specified. I saved the merged bams
under the pshell directory: `NGS Analysis results/rawdata/Darwins_Daisys/merged_bams_jb`. This will allow me to keep the
bam files separate from the ones generated by Gagan.

I merged the excov files from SGSGeneloss, graph.csv and stats.txt files for each sample above to the directory: 
`NGS Analysis results/rawdata/Darwins_Daisys/sgsgeneloss_out_jb/`. This folder contains 103 files that consist of 
the following:

* 1x merged.excov file for each sample.
* 1x stats.txt file for each sample.
* 1x graph.csv file for each sample.

A singular chrs.csv file containing the chromosome order for each sample (will be the same for each sample). The
directory contains 103 files in total.

### Analysis of Alignment results.
Before checking the SGSGeneloss results, I ran [this](scripts/bam_stats/visualise_bam_stats.py) parser script to 
summarise the alignment statistics. Due to the results Gagan had with her SGSGeneLoss run, I wanted to check that each
sample had aligned with a high percentage. After analysis of the results I concluded that all samples looked good, with 
the lowest mapping rate of the 34 samples at 93.14 %, with a mean alignment percentage of 94.7% between all samples.

All samples seemed to have a good read count, with the lowest sample scoring 127M total reads and an average of around
200M reads.

## Analysis of SGSGeneLoss results (07/07/2025) - ().
I ran SGSGeneloss on the merged bam files and ran the outputs through this [script](scripts/sgsgeneloss/merge_excovs.py)
to merge the excov files for each sample. I then generated a report using the output files using this
[script](scripts/sgsgeneloss/plot_stats.py) the final report for this run of the SGSGeneLoss can be viewed
[here](reports/sgsgeneloss_report.html).

The initial SGSGeneloss results were promising, and after confirming with Dave and Mitch we agreed that all the 
samples in the cohort contain suitable read depth/coverage to provide accurate presence/absence variation results.

### Clustering of samples.
The initial PCA results of the PAV matrix during the generation of the report above showed that there seems to be some
expected clustering of samples. I spent some time into looking into what would be the best method of comparing
clustering results for this dataset (binary). And decided to replace the PCA plot with a UMAP plot to look at sample
clustering.

The figure below shows UMAP analysis of the 34 sample PAV data, with dots colored according to the island the samples 
were taken from:

![UMAP Plot of PAV matrix.](plots/sgsgeneloss/pav_island_umap.png)

The plot shows some rough clustering between samples from different islands, although there are some samples from
those islands that do not cluster well. I also generated a plot that maps the sample longitude and latitude to thier
co-ordinates around the GalÃ¡pagos Islands themselves. Here is the plot:

![Co-ordinate location of samples.](plots/sgsgeneloss/galapagos_map_with_samples.png)

## Functional annotation (09/07/2025) - (11/07/2025).
In order to provide a more informative analysis of which genes are retained/lost in each sample. I need to add 
additional annotation to the maker-generated gff3 annotation file used to run SGSGeneloss. In order to provide 
additional functional annotation, I extracted CDS and protein sequences (as fasta files) 
of each feature in the gff3 file using the `gffread` package. 

I then used the `diamond/2.1.12` to do an initial screen of the fasta files for high quality hits against the
uniprot database. In order to output a file that contained taxonomy information, I needed to provide some additional
files to the diamond database build. I followed the instructions 
[here](https://github.com/bbuchfink/diamond/wiki/3.-Command-line-options) in order to build that database using a
file to map protein accessions to taxonomy accessions.

(Protein -> Taxonomy) mapping file:
`wget https://ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.FULL.gz` 

Taxon information for the database was obtained using:
`wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz`. The database was built using the following bash command

Then with `diamond`:
```bash
diamond makedb \
  --in uniprot_sprot.fasta \
  --db uniprot_sprot \
  --taxonmap prot.accession2taxid.FULL \
  --taxonnodes nodes.dmp \
  --taxonnames names.dmp
```
Before running the blast command I ran [this](/scripts/functional_annotation/clean_proteins.py) script on the gffreads 
`protein.fa` output file to clean the sequences and make sure any invalid characters were removed from the dataset
passed in to diamond. 

Finally, I ran this code to run a `blastp` search against the uniprot_sprot diamond database generated above using this
script: [this](/scripts/functional_annotation/diamond_blast.sh) script. To generate the output file: 
`diamond_results_protein.tsv`. That was transferred to my local PC for data analysis.

Before continuing the analysis I wanted to get a good idea of the number of proteins that had significant blast hits.
[this](/scripts/functional_annotation/annotation_analysis.py) python file was used to generate some statistics for 
the functional annotation results of the initial diamond blast hits.

Here are some summary statistics for the initial diamond results:
* Diamond results contained blastp results for 36070 features (out of a total of 43093 in the original gff file).
* All 36070 queries were unique.
* Diamond hits by "identity" are normally distributed around the 60% sequence similarity. 
* Majority of top blast hits for the gff3 file against uniprot_sprot database were from plant kingdom "Viridiplantae".
* 23506/43093 features in the gff3 file returned a top hit against "Arabidopsis".

### Getting GO terms for each gene (11/07/2025) - (14/07/2025).
I cannot retrieve GO terms directly from the diamond blastp , but I can screen the protein ID's against uniprot to
obtain a list of GO terms. 

In order to apply a computationally efficient approach I downloaded the Uniprot GO term mapping file to Setonix using:
`https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/idmapping/idmapping_selected.tab.gz`

I then created a list of unique accession_id's from the output of the `diamond blastp` file:
[diamond_results.tsv](/data/functional_annotation/diamond_results_protein.tsv). Using the command:
`cut -f2 diamond_results.tsv | cut -d'|' -f2 | sort -u > accessions.txt`

Finally, I used `gunzip`'d the mapping file and then screened it for the unique accessions created above using the
command: 
```bash
awk -F'\t' '
  NR==FNR {acc[$1]; next}
  ($1 in acc) && $7 != "" {
    go[$1] = go[$1] ? go[$1]";"$7 : $7
  }
  END {
    for (id in acc) {
      print id "\t" (go[id] ? go[id] : "NA")
    }
  }
' accessions.txt idmapping_selected.tab > go_mapping.tsv
```
The above command will read all accessions from `accessions.txt`, extract all the GO terms for those accessions,
group multiple GO terms per accession as separated by a semicolon, ensures that every accession from the list appears
in the output with "NA" if no GO terms are found.

Finally, I confirmed that both files had the same number of lines with `wc -l`. This check confirmed that out of the
43093 structural features identified in the gff file, 11771 were mapped to GO-terms using the top blastp hit via
`diamond blastp`.

I merged the file containing the GO terms and accessions to my diamond blast file using an inner join and the 
Python script [here](/scripts/functional_annotation/merge_go_terms.py). I wrote the output of this file to
[here](/data/functional_annotation/go_merged_diamond_results_uniprot.tsv).

I will use this file to do some GO enrichment analysis using the Go terms from uniprot as well as the presence/absence
variation matrix [here](/data/sgsgeneloss/pav_matrix.csv)

## Go enrichment analysis (14/07/2025) - (15/07/2025):
First of all I thought it would be a good idea to get an overview of which Go terms are most common in my dataset,
this will allow me to establish a functional landscape and give a broad overview of which go terms are most 
represented by the hits.

[This](/scripts/functional_annotation/analyse_go_terms.py) Python script was used to generate all go-term related 
analysis. It also contains the code used to generate a human-readable GO-id : Go-term readable file, that provides a 
human-readable mapping of each go-id that can be used to make any subsequent figures more readable. The input file for 
this was obtained from this link: `https://geneontology.org/docs/download-ontology/`. The `go-basic.obo` was downloaded
and parsed with python to create the mapping file.

I generated two plots to get a general overview of the top GO terms overall, and by namespace within the overall 
dataset (all 11771 hits). These plots are shown below:

![Top GO ID's by frequency (all hits).](/plots/functional_annotation/diamond_hits_all_most_freq_go_terms.png)
![Top GO ID's by namespace (all hits.)](/plots/functional_annotation/diamond_hits_all_most_freq_go_terms_by_namespace.png)

I then filtered the Go-annotated diamond blastp results dataframe into two groups:

1) `Core` hits - Out of 41105 core genes/features in the pav dataframe, I was able to generate hits for 34562
2) `Non-core` hits - Out of 1988, I was able to generate hits for 1508.

These hits were generated using the uniprot-sprot database, cross-referenced with associated GO terms. I then reran 
top GO-terms by count for the `core` and `non-core` gene list. The results are shown below:

Background dataset (all-genes):
![Background dataset - Top Go terms by count.](/plots/functional_annotation/diamond_hits_all_most_freq_go_terms.png)

Core genes (Genes marked present in all samples.)
![Core gene set.](/plots/functional_annotation/diamond_hits_core_most_freq_go_terms.png)

Non-core genes (genes marked as absent in at least one sample).
![Non-core gene set.](/plots/functional_annotation/diamond_hits_noncore_most_freq_go_terms.png)

The results above give a good indication as to the results we might expect to see through a gene enrichment analysis.
I will run a gene enrichment analysis using the `goatools` python package, which I was able to install with conda on
my local pc. The script use to run the GO enrichment analysis is [here](/scripts/functional_annotation/go_enrichment.py)

### Update 18/07/2025 - 22/07/2025.
After Discussing with Dave, I will have a more thorough look into absent genes that are associated with the 
terpenoid pathway. The number 1 scoring go term in the non-core gene lists. This 
[script](/scripts/functional_annotation/filter_by_go.py) was used to complete that analysis. Taking these results,
I also completed a small review to assess future directions that we could take the project. This work is located
[here](/reports/sgsgeneloss_gene_analysis.docx)

## Running masurca on unmapped reads (16/07/2025) -(21/07/2025).
Masurca prefers untrimmed reads for the assembly. In order to collect these I had to download the raw .fastqz files from
`pshell` and merge them by sample using this [script](/scripts/masurca/merge_daisy_raws_fastq.sh). Once I had merged the
reads, I then extracted the untrimmed versions of the unmapped reads using this 
[script](scripts/masurca/extract_unmapped.sh) to retrieve the untrimmed version of the unmapped reads from the raw
fastq files. The unmapped_untrimmed reads were uploaded to pshell at the directory:
`NGS Analysis Results/rawdata/Darwins Daisy/untrimmed_unmapped_fastqs_jb`

I estimated the average insert size and STDEV of a few samples using `bbmap merge` to give the following results:

* AVG insert size: 231
* AVG insert STDEV: 33.2

Finally, I used this [script](/scripts/masurca/run_masurca.sh) to run the masurca assembly. With the maximum single 
`work` node allocation of 128CPUS and 230GB mem. This run failed so I reran using the `highmem` partition, reduced the 
number of cpu's down to 64 and upped the memory requirements to 300GB. This run completed. I copied the masurca output 
files to `pshell` at the following directory:

`NGS Analysis Results/rawdata/Darwins_daisy/masurca_output_jb`

I used the `quast` programme, installed via `conda` to analyse the results of the _de novo_ alignment. The results are
stored [here](/reports/masurca_quast_report.html).

Here are some summaries of the initial alignment:

* Number of contigs: 171908
* Contigs over 1000BP: 83859
* Contigs over 5000BP: 9357
* Contigs over 10000BP: 1212

### Masurca run quality control (22/07/2025) - .
I sent the masurca report to Mitch and Dave and we agreed the overall number of contigs is high. I am going to run
a filter for mitochondrial and chloroplast DNA on the assembly to assess and filter how many contigs likely belong to
organelles and then can move on to look for other contamination, such as bacteria.

In order to see which contigs belong to mito/chloroplasts I will run a `blastn` search against mitochondria and 
chloroplast complete genomes from the following NCBI accessions for the common sunflower using this 
against these reference fastas:

* NC_007977.1 (chloroplast).
* NC_023337.1 (mitochondria).

The following code shows the process used to remove contigs that were flagged as mitochondrial/chloroplast DNA from the
primary masurca scaffold using blastn and entrez-direct packages:

```bash
efetch -db nucleotide -d <ID> -format fasta > <NCBI_reference>.fasta
makeblastdb -in <NCBI_reference>.fasta -dbtype nucl -out <NCBI_reference>_db
blastn -query <masurca_assembly>.fasta -db <NCBI_reference>_db \
       -outfmt '6 qseqid sseqid pident length evalue bitscore' \
       -evalue 1e-10 \
       -out <hits>.tsv \
       -num_threads 16
```

Once I had generated blast databases for both NCBI accessions. I then ran the following code to create a list of
unique hits using the two .tsv files to create a file: <all_organelle_hits>. Then used seqtk `grep` to remove these hits
from the original masurca assembly.

```bash
cat <hits1?.tsv> <hits2.tsv> | sort | uniq > organelle_contigs.txt
seqkit grep organelle_contigs.tsv primary_masurca_assembly.fasta > filtered_masurca_assembly.fasta

# A few checks to make sure that the primary masurca fasta file was actually filtered.
grep -c primary_masurca_assembly.fasta
grep -c filtered_masurca_assembly.fasta
```

I also saved the mitochondria and chloroplast filtered masurca assembly to `pshell` at the directory:
`NGS Analysis Results/rawdata/darwins_daisies/masurca_output_jb/filtered.genome.scf.fasta` I also saved the list of 
contigs that were filtered out from the masurca assembly under the same folder with the filename 
`sunflower_organelle_contigs.txt` I also saved the fasta sequences as `sunflower_chloro.fast` and `sunflower_mito.fasta`
under the same folder.

## Alignment of Filtered masurca assembly to the reference genome (23/07/2025) -  
The next step in the generation of the pan-genome involved remapping the filtered masurca assembly to the original
_scalesia_ reference file. This can be downloaded from pshell here: 
`NGS analysis results/rawdata/darwins daisies/REF_FILE/.fasta`.

Once I had downloaded this file, I used it as the reference, and aligned the masurca contigs that had been filtered to
remove mitochondrial/chloroplast DNA using this [script](/scripts/masurca/run_minimap.slrm).
See below for some basic mapping statistics generated using the `samtools stats` command:

* Total contigs: 313 873.
* Mapped contigs: 256 090
* Unmapped contigs: 57 783
* Average contig length: 1092BP
* ~85% of contigs mapped.
* 20% mismatch rate. - High because this alignment is from multiple _scalasia_ species.

I then extracted the unmapped reads from this alignment using the following code, to generate a list of completely 
unmapped contigs:

```bash
samtools fastq -f 4 alignment.bam > unmapped_reads.fastq
seqtk seq -a unmapped_reads.fastq > unmapped_reads.fasta
```

The unmapped_reads.fasta generated by `samtools` contains 57 783 contigs. 

Finally, I used this array [script](/scripts/masurca/run_blastn_array.slrm)
and this code snippet (run after all jobs finished). To generate a `.tsv` file containing the top `blastn` hit for each
unmapped contig. This was done to get an idea of whether the unmapped (novel) contigs still contained contamination,
or whether the majority was still flagged as plant material.

