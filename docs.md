# Darwin's Daisy Project, Taken over from Gagan 20/06/2025.
## Inital handover notes.

I met with Dave, A few of the postdocs and Gagan to discuss handover of the project on the 20/06/2025. To discuss the 
further work needed for the project. We came up with a list of tasks that needed doing. These are listed below:

* Generate a plot showing number of genes called as "present" against read depth for each sample. The curve should look 
like a cliff, with read depth tapering off as you move from right to left through the curve. This will help us to see 
which samples need fixing.

* Run maker annotation with the original genome. Will need to look into how the original genome was annotated. For
help with maker, I can ask Tessa. She has lot's of experience with it.

* Call presence/absence annotation. This can be done in parallel to maker.
* Gene ontology analysis.
* Map unmapped contigs back to the reference genome (the unmapped contigs have already been filtered). - Apparently 
Teng can help me with this.
* Look for a multiqc report to check quality of each sample and ensure that we are confident with all samples before 
moving forward.

## Assessing quality of samples.
I downloaded the multiqc report Gagan generated to look over and see if it could explain why some samples are showing
a large amount of gene loss. The multiqc report is [here](reports/initial_multiqc_report.html). The report showed that
despite passing quality thresholds, the number of total reads for some samples were very low, this means that the
depth of coverage of the genomes for these samples may not be good enough to cover all of the present genes and hence 
would lead to the large loss of genes Gagan observed in her thesis. This was confirmed in the 
[mapping report](reports/mapping_multiqc_report.html) that showed that all samples had a high rate of alignment but 
likely do not have a good depth of coverage.

To a suitable threshold for suitable depth, The read depth plot below will be informative. It may even be informative
to show raw read count per sample against number of genes called as present.

## Generating present genes / read depth plot.
Gagan's sorted .bam folder only contains 26 bam files. I am going to use the raw_bams to complete the merge, as 
there should be 35 samples according to the [metadata](metadata/raw_sample_metadata.xlsx)

To Generate the plot I will complete the following workflow:
* Get read depth per .bam file using `samtools view -c -F 260 <sample>.bam`. This will get the number of mapped reads.
* Count genes per .ba, using `featureCounts -a .gff -o counts.txt <sample>.bam`
* Define "Present". I.e what threshold should determine if a gene is classed as present or not.
* Combine results into a table with columns `sample`, `Mapped read count`, and `Genes present`.
* Plot with Python.

Things to think about.
To assess how gene detection behaves at lower depths within the same sample, I could take a subsample of each of the 
.bams.

### Update 30/06/2025.
I finished merging the bam files and running SGSGeneLoss on the samples on the date specified. I saved the merged bams
under the pshell directory: `NGS Analysis results/rawdata/Darwins_Daisys/merged_bams_jb`. This will allow me to keep the
bam files separate from the ones generated by Gagan.

I merged the excov files from SGSGeneloss, graph.csv and stats.txt files for each sample above to the directory: 
`NGS Analysis results/rawdata/Darwins_Daisys/sgsgeneloss_out_jb/`. This folder contains 103 files that consist of 
the following:

* 1x merged.excov file for each sample.
* 1x stats.txt file for each sample.
* 1x graph.csv file for each sample.

A singular chrs.csv file containing the chromosome order for each sample (will be the same for each sample). The
directory contains 103 files in total.

### Analysis of Alignment results.
Before checking the SGSGeneloss results, I ran [this](scripts/bam_stats/visualise_bam_stats.py) parser script to 
summarise the alignment statistics. Due to the results Gagan had with her SGSGeneLoss run, I wanted to check that each
sample had aligned with a high percentage. After analysis of the results I concluded that all samples looked good, with 
the lowest mapping rate of the 34 samples at 93.14 %, with a mean alignment percentage of 94.7% between all samples.

All samples seemed to have a good read count, with the lowest sample scoring 127M total reads and an average of around
200M reads.

## Analysis of SGSGeneLoss results (07/07/2025) - ().
I ran SGSGeneloss on the merged bam files and ran the outputs through this [script](scripts/sgsgeneloss/merge_excovs.py)
to merge the excov files for each sample. I then generated a report using the output files using this
[script](scripts/sgsgeneloss/plot_stats.py) the final report for this run of the SGSGeneLoss can be viewed
[here](reports/sgsgeneloss_report.html).

The initial SGSGeneloss results were promising, and after confirming with Dave and Mitch we agreed that all the 
samples in the cohort contain suitable read depth/coverage to provide accurate presence/absence variation results.

### Clustering of samples.
The initial PCA results of the PAV matrix during the generation of the report above showed that there seems to be some
expected clustering of samples. I spent some time into looking into what would be the best method of comparing
clustering results for this dataset (binary). And decided to replace the PCA plot with a UMAP plot to look at sample
clustering.

The figure below shows UMAP analysis of the 34 sample PAV data, with dots colored according to the island the samples 
were taken from:

![UMAP Plot of PAV matrix.](plots/sgsgeneloss/pav_island_umap.png)

The plot shows some rough clustering between samples from different islands, although there are some samples from
those islands that do not cluster well. I also generated a plot that maps the sample longitude and latitude to thier
co-ordinates around the GalÃ¡pagos Islands themselves. Here is the plot:

![Co-ordinate location of samples.](plots/sgsgeneloss/galapagos_map_with_samples.png)

## Functional annotation (09/07/2025) - (11/07/2025).
In order to provide a more informative analysis of which genes are retained/lost in each sample. I need to add 
additional annotation to the maker-generated gff3 annotation file used to run SGSGeneloss. In order to provide 
additional functional annotation, I extracted CDS and protein sequences (as fasta files) 
of each feature in the gff3 file using the `gffread` package. 

I then used the `diamond/2.1.12` to do an initial screen of the fasta files for high quality hits against the
uniprot database. In order to output a file that contained taxonomy information, I needed to provide some additional
files to the diamond database build. I followed the instructions 
[here](https://github.com/bbuchfink/diamond/wiki/3.-Command-line-options) in order to build that database using a
file to map protein accessions to taxonomy accessions.

(Protein -> Taxonomy) mapping file:
`wget https://ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.FULL.gz` 

Taxon information for the database was obtained using:
`wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz`. The database was built using the following bash command

Then with `diamond`:
```bash
diamond makedb \
  --in uniprot_sprot.fasta \
  --db uniprot_sprot \
  --taxonmap prot.accession2taxid.FULL \
  --taxonnodes nodes.dmp \
  --taxonnames names.dmp
```
Before running the blast command I ran [this](/scripts/functional_annotation/clean_proteins.py) script on the gffreads 
`protein.fa` output file to clean the sequences and make sure any invalid characters were removed from the dataset
passed in to diamond. 

Finally, I ran this code to run a `blastp` search against the uniprot_sprot diamond database generated above using this
script: [this](/scripts/functional_annotation/diamond_blast.sh) script. To generate the output file: `diamond_results_protein.tsv`. That was transferred to my local
PC for data analysis.

Before continuing the analysis I wanted to get a good idea of the number of proteins that had significant blast hits.
[this](/scripts/functional_annotation/annotation_analysis.py) python file was used to generate some statistics for 
the functional annotation results of the initial diamond blast hits.

Here are some summary statistics for the initial diamond results:
* Diamond results contained blastp results for 36070 features (out of a total of 43093 in the original gff file).
* All 36070 queries were unique.
* Diamond hits by "identity" are normally distributed around the 60% sequence similarity. 
* Majority of top blast hits for the gff3 file against uniprot_sprot database were from plant kingdom "Viridiplantae".
* 23506/43093 features in the gff3 file returned a top hit against "Arabidopsis".

### Getting GO terms for each gene (11/07/2025) - (14/07/2025).
I cannot retrieve GO terms directly from the diamond blastp , but I can screen the protein ID's against uniprot to
obtain a list of GO terms. 

In order to apply a computationally efficient approach I downloaded the Uniprot GO term mapping file to Setonix using:
`https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/idmapping/idmapping_selected.tab.gz`

I then created a list of unique accession_id's from the output of the `diamond blastp` file:
[diamond_results.tsv](/data/functional_annotation/diamond_results_protein.tsv). Using the command:
`cut -f2 diamond_results.tsv | cut -d'|' -f2 | sort -u > accessions.txt`

Finally, I used `gunzip`'d the mapping file and then screened it for the unique accessions created above using the
command: 
```bash
awk -F'\t' '
  NR==FNR {acc[$1]; next}
  ($1 in acc) && $7 != "" {
    go[$1] = go[$1] ? go[$1]";"$7 : $7
  }
  END {
    for (id in acc) {
      print id "\t" (go[id] ? go[id] : "NA")
    }
  }
' accessions.txt idmapping_selected.tab > go_mapping.tsv
```
The above command will read all accessions from `accessions.txt`, extract all the GO terms for those accessions,
group multiple GO terms per accession as separated by a semicolon, ensures that every accession from the list appears
in the output with "NA" if no GO terms are found.

Finally, I confirmed that both files had the same number of lines with `wc -l`. This check confirmed that out of the
43093 structural features identified in the gff file, 11771 were mapped to GO-terms using the top blastp hit via
`diamond blastp`.

I merged the file containing the GO terms and accessions to my diamond blast file using an inner join and the 
Python script [here](/scripts/functional_annotation/merge_go_terms.py). I wrote the output of this file to
[here](/data/functional_annotation/go_merged_diamond_results_uniprot.tsv).

I will use this file to do some GO enrichment analysis using the Go terms from uniprot as well as the presence/absence
variation matrix [here](/data/sgsgeneloss/pav_matrix.csv)

## Go enrichment analysis (14/07/2025) - (15/07/2025):
First of all I thought it would be a good idea to get an overview of which Go terms are most common in my dataset,
this will allow me to establish a functional landscape and give a broad overview of which go terms are most 
represented by the hits.

[This](/scripts/functional_annotation/analyse_go_terms.py) Python script was used to generate all go-term related 
analysis. It also contains the code used to generate a human-readable GO-id : Go-term readable file, that provides a 
human-readable mapping of each go-id that can be used to make any subqequent figures more readable. The input file for 
this was obtained from this link: `https://geneontology.org/docs/download-ontology/`. The `go-basic.obo` was downloaded
and parsed with python to create the mapping file.

I generated two plots to get a general overview of the top GO terms overall, and by namespace within the overall 
dataset (all 11771 hits). These plots are shown below:

![Top GO ID's by frequency (all hits).](/plots/functional_annotation/diamond_hits_all_most_freq_go_terms.png)
![Top GO ID's by namespace (all hits.)](/plots/functional_annotation/diamond_hits_all_most_freq_go_terms_by_namespace.png)

I then filtered the Go-annotated diamond blastp results dataframe into two groups:

1) `Core` hits - Out of 41105 core genes/features in the pav dataframe, I was able to generate hits for 34562
2) `Non-core` hits - Out of 1988, I was able to generate hits for 1508.

These hits were generated using the uniprot-sprot database, cross-referenced with associated GO terms. I then reran 
top GO-terms by count for the `core` and `non-core` gene list. The results are shown below:

Background dataset (all-genes):
![Background dataset - Top Go terms by count.](/plots/functional_annotation/diamond_hits_all_most_freq_go_terms.png)

Core genes (Genes marked present in all samples.)
![Core gene set.](/plots/functional_annotation/diamond_hits_core_most_freq_go_terms.png)

Non-core genes (genes marked as absent in at least one sample).
![Non-core gene set.](/plots/functional_annotation/diamond_hits_noncore_most_freq_go_terms.png)

The results above give a good indication as to the results we might expect to see through a gene enrichment analysis.
I will run a gene enrichment analysis using the `goatools` python package, which i was able to install with conda on
my local pc. The script use to run the GO enrichment analysis is [here](/scripts/functional_annotation/go_enrichment.py)

## Running masurca on unmapped reads (16/07/2025) -().
Masurca prefers untrimmed reads for the assembly. In order to collect these I had to download the raw .fastqz files from
`pshell` and merge them by sample using this [script](). Once I had merged the reads, I then extracted the untrimmed
versions of the unmapped reads using this [script](). 

I estimated the average insert size and STDEV of a few samples using `bbmap merge` to give the following results:

Finally, I used this [script]() to run the masurca assembly. 

