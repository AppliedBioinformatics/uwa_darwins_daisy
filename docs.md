# Darwin's Daisy Project, Taken over from Gagan 20/06/2025.
## Inital handover notes.

I met with Dave, A few of the postdocs and Gagan to discuss handover of the project on the 20/06/2025. To discuss the 
further work needed for the project. We came up with a list of tasks that needed doing. These are listed below:

* Generate a plot showing number of genes called as "present" against read depth for each sample. The curve should look 
like a cliff, with read depth tapering off as you move from right to left through the curve. This will help us to see 
which samples need fixing.

* Run maker annotation with the original genome. Will need to look into how the original genome was annotated. For
help with maker, I can ask Tessa. She has lot's of experience with it.

* Call presence/absence annotation. This can be done in parallel to maker.
* Gene ontology analysis.
* Map unmapped contigs back to the reference genome (the unmapped contigs have already been filtered). - Apparently 
Teng can help me with this.
* Look for a multiqc report to check quality of each sample and ensure that we are confident with all samples before 
moving forward.

## Assessing quality of samples.
I downloaded the multiqc report Gagan generated to look over and see if it could explain why some samples are showing
a large amount of gene loss. The multiqc report is [here](reports/initial_multiqc_report.html). The report showed that
despite passing quality thresholds, the number of total reads for some samples were very low, this means that the
depth of coverage of the genomes for these samples may not be good enough to cover all of the present genes and hence 
would lead to the large loss of genes Gagan observed in her thesis. This was confirmed in the 
[mapping report](reports/mapping_multiqc_report.html) that showed that all samples had a high rate of alignment but 
likely do not have a good depth of coverage.

To a suitable threshold for suitable depth, The read depth plot below will be informative. It may even be informative
to show raw read count per sample against number of genes called as present.

## Generating present genes / read depth plot.
Gagan's sorted .bam folder only contains 26 bam files. I am going to use the raw_bams to complete the merge, as 
there should be 35 samples according to the [metadata](metadata/raw_sample_metadata.xlsx)

To Generate the plot I will complete the following workflow:
* Get read depth per .bam file using `samtools view -c -F 260 <sample>.bam`. This will get the number of mapped reads.
* Count genes per .ba, using `featureCounts -a .gff -o counts.txt <sample>.bam`
* Define "Present". I.e what threshold should determine if a gene is classed as present or not.
* Combine results into a table with columns `sample`, `Mapped read count`, and `Genes present`.
* Plot with Python.

Things to think about.
To assess how gene detection behaves at lower depths within the same sample, I could take a subsample of each of the 
.bams.

### Update 30/06/2025.
I finished merging the bam files and running SGSGeneLoss on the samples on the date specified. I saved the merged bams
under the pshell directory: `NGS Analysis results/rawdata/Darwins_Daisys/merged_bams_jb`. This will allow me to keep the
bam files separate from the ones generated by Gagan.

I merged the excov files from SGSGeneloss, graph.csv and stats.txt files for each sample above to the directory: 
`NGS Analysis results/rawdata/Darwins_Daisys/sgsgeneloss_out_jb/`. This folder contains 103 files that consist of 
the following:

* 1x merged.excov file for each sample.
* 1x stats.txt file for each sample.
* 1x graph.csv file for each sample.

A singular chrs.csv file containing the chromosome order for each sample (will be the same for each sample). The
directory contains 103 files in total.

### Analysis of Alignment results.
Before checking the SGSGeneloss results, I ran [this](scripts/bam_stats/visualise_bam_stats.py) parser script to 
summarise the alignment statistics. Due to the results Gagan had with her SGSGeneLoss run, I wanted to check that each
sample had aligned with a high percentage. After analysis of the results I concluded that all samples looked good, with 
the lowest mapping rate of the 34 samples at 93.14 %, with a mean alignment percentage of 94.7% between all samples.

All samples seemed to have a good read count, with the lowest sample scoring 127M total reads and an average of around
200M reads.

## Analysis of SGSGeneLoss results (07/07/2025) - ().
I ran SGSGeneloss on the merged bam files and ran the outputs through this [script](scripts/sgsgeneloss/merge_excovs.py)
to merge the excov files for each sample. I then generated a report using the output files using this
[script](scripts/sgsgeneloss/plot_stats.py) the final report for this run of the SGSGeneLoss can be viewed
[here](reports/sgsgeneloss_report.html).

The initial SGSGeneloss results were promising, and after confirming with Dave and Mitch we agreed that all the 
samples in the cohort contain suitable read depth/coverage to provide accurate presence/absence variation results.

### Clustering of samples.
The initial PCA results of the PAV matrix during the generation of the report above showed that there seems to be some
expected clustering of samples. I spent some time into looking into what would be the best method of comparing
clustering results for this dataset (binary). And decided to replace the PCA plot with a UMAP plot to look at sample
clustering.

The figure below shows UMAP analysis of the 34 sample PAV data, with dots colored according to the island the samples 
were taken from:

![UMAP Plot of PAV matrix.](plots/sgsgeneloss/pav_island_umap.png)

The plot shows some rough clustering between samples from different islands, although there are some samples from
those islands that do not cluster well. It would be good to plot the islands in the Galapagos so that I can provide some
more context to the clusters that are generated.

## Functional annotation (09/07/2025) - ().
In order to provide a more informative analysis of which genes are retained/lost in each sample. I need to add 
additional annotation to the maker-generated gff3 annotation file used to run SGSGeneloss. In order to provide 
additional functional annotation, I extracted CDS and protein sequences (as fasta files) 
of each feature in the gff3 file using the `gffread` package. 

I then used the `diamond/2.1.12` to do an initial screen of the fasta files for high quality hits against the
uniprot database. In order to output a file that contained taxonomy information, I needed to provide some additional
files to the diamond database build. I followed the instructions 
[here](https://github.com/bbuchfink/diamond/wiki/3.-Command-line-options) in order to build that database using a
file to map protein accessions to taxonomy accessions.

(Protein -> Taxonomy) mapping file:
`wget https://ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.FULL.gz` 

Taxon information for the database was obtained using:
`wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz`. The database was built using the following bash command

Then with `diamond`:
```bash
diamond makedb \
  --in uniprot_sprot.fasta \
  --db uniprot_sprot \
  --taxonmap prot.accession2taxid.FULL \
  --taxonnodes nodes.dmp
  --taxonnames names.dmp
```

Before running the blast command I ran [this](/scripts/functional_annotation/clean_proteins.py) script on the gffreads 
`protein.fa` output file to clean the sequences and make sure any invalid characters were removed from the dataset
passed in to diamond. 

Finally, I ran this code to run a `blastp` search against the uniprot_sprot diamond database generated above using this
script: [this](/scripts/functional_annotation/diamond_blast.sh) script. To generate the output file: `diamond_results_protein.tsv`. That was transferred to my local
PC for data analysis.

Before continuing the analysis I wanted to get a good idea of the number of proteins that had significant blast hits.
[this]() python file was used to generate some statistics for the functional annotation results of the initial diamond
blast hits.

Here are some summary statistics for the initial diamond results:
* Diamond results contained blastp results for 36070 features (out of a total of 43093 in the original gff file).
* All 36070 queries were unique.
* Diamond hits by "identity" are normally distributed around the 60% sequence similarity. 
* Majority of top blast hits for the gff3 file against uniprot_sprot database were from plant kingdom "Viridiplantae".
* 23506/43093 features in the gff3 file returned a top hit against "Arabidopsis".

### Getting GO terms for each gene.
I cannot retrieve GO terms directly from the diamond blastp , but I can screen the protein ID's against uniprot to
obtain a list of GO terms. 

In order to apply a computationally efficient approach I downloaded the Uniprot GO term mapping file to Setonix using:
``

I then created a list of accessions_id's from the output of the `diamond blastp` file:
[diamond_results.tsv](/data/functional_annotation/diamond_results_protein.tsv). Using the command:
`cut -f2 diamond_results.tsv | cut -d'|' -f2 | sort -u > accessions.txt`

Finally, I used `zgrep` to pull out the GO annotation data for each of the accessions generated in `accessions.txt` using
the command: 
`zgrep -Ff accessions.txt idmapping.dat.gz | awk '$2 == "GO" || $2 == "KEGG" || $2 == "Pfam"' > filtered_annotations.tsv`












